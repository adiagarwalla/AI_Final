DecisionTree.java has 2 constructors, one for use in constructing a full decision tree and one for use in constructing a height limited decision tree for use in AdaBoost. The full decision tree takes a DataSet as a parameter, and saves (1-alpha) of training examples for use in constructing the tree, and reserves alpha of the training examples for validation purposes in pruning.  

DecisionTree makes use of a helper class, Node.java. Each node consists of a classification, an attribute, and an array of subtrees called children. A DecisionTree merely consists a Node root. DecisionTree.predict() works by just following the child links from the root all the way until it reaches a leaf Node, and returning the last classification found on the path. 

To prune our tree, we have a recursive helper method called prune(Node node, List<Integer> validation) which takes the Node to possibly prune, as well as a list of examples used for validation purposes. We calculate the error e in the validation set without pruning, as well as the error e' in the validation set with pruning, and prune if e' <= e. If we do not prune node, then we call prune() on node's children. 